Project Structure
The project is structured as follows:

0-rnn_encoder.py: Implements the RNN Encoder class.
1-self_attention.py: Implements the Self Attention class.
2-rnn_decoder.py: Implements the RNN Decoder class.
4-positional_encoding.py: Provides the function to calculate positional encodings.
5-sdp_attention.py: Implements the scaled dot product attention function.
6-multihead_attention.py: Implements the Multi-Head Attention class.
7-transformer_encoder_block.py: Implements an Encoder Block of a Transformer.
8-transformer_decoder_block.py: Implements a Decoder Block of a Transformer.
9-transformer_encoder.py: Implements the Transformer Encoder class.
10-transformer_decoder.py: Implements the Transformer Decoder class.
11-transformer.py: Implements the complete Transformer network.